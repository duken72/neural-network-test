{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AlexNet-pytorch.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM46FkIaqqvf0WP110R3LiO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"a1yZM-HRWCJG"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","source":["def get_train_valid_loader(data_dir, batch_size, augment, random_seed,\n","                           valid_size=0.1, shuffle=True):\n","    normalize = transforms.Normalize(\n","        mean=[0.4914, 0.4822, 0.4465],\n","        std=[0.2023, 0.1994, 0.2010],\n","    )\n","\n","    # define transforms\n","    valid_transform = transforms.Compose([\n","            transforms.Resize((227,227)),\n","            transforms.ToTensor(),\n","            normalize,\n","    ])\n","    if augment:\n","        train_transform = transforms.Compose([\n","            transforms.RandomCrop(32, padding=4),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            normalize,\n","        ])\n","    else:\n","        train_transform = transforms.Compose([\n","            transforms.Resize((227,227)),\n","            transforms.ToTensor(),\n","            normalize,\n","        ])\n","\n","    # load the dataset\n","    train_dataset = datasets.CIFAR10(\n","        root=data_dir, train=True,\n","        download=True, transform=train_transform,\n","    )\n","\n","    valid_dataset = datasets.CIFAR10(\n","        root=data_dir, train=True,\n","        download=True, transform=valid_transform,\n","    )\n","\n","    num_train = len(train_dataset)\n","    indices = list(range(num_train))\n","    split = int(np.floor(valid_size * num_train))\n","\n","    if shuffle:\n","        np.random.seed(random_seed)\n","        np.random.shuffle(indices)\n","\n","    train_idx, valid_idx = indices[split:], indices[:split]\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=batch_size, sampler=train_sampler)\n"," \n","    valid_loader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n","\n","    return (train_loader, valid_loader)\n","\n","\n","def get_test_loader(data_dir, batch_size, shuffle=True):\n","    normalize = transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225],\n","    )\n","\n","    # define transform\n","    transform = transforms.Compose([\n","        transforms.Resize((227,227)),\n","        transforms.ToTensor(),\n","        normalize,\n","    ])\n","\n","    dataset = datasets.CIFAR10(\n","        root=data_dir, train=False,\n","        download=True, transform=transform,\n","    )\n","\n","    data_loader = torch.utils.data.DataLoader(\n","        dataset, batch_size=batch_size, shuffle=shuffle\n","    )\n","\n","    return data_loader\n","\n","\n","# CIFAR10 dataset \n","train_loader, valid_loader = get_train_valid_loader(data_dir = './data',                                      batch_size = 64,\n","                       augment = False, random_seed = 1)\n","\n","test_loader = get_test_loader(data_dir = './data', batch_size = 64)"],"metadata":{"id":"qZTTyBCZWNLB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AlexNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(AlexNet, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n","            nn.BatchNorm2d(96),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 3, stride = 2))\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 3, stride = 2))\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(384),\n","            nn.ReLU())\n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(384),\n","            nn.ReLU())\n","        self.layer5 = nn.Sequential(\n","            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 3, stride = 2))\n","        self.fc = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(9216, 4096),\n","            nn.ReLU())\n","        self.fc1 = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU())\n","        self.fc2= nn.Sequential(\n","            nn.Linear(4096, num_classes))\n","        \n","    def forward(self, x):\n","        y = self.layer1(x)\n","        y = self.layer2(y)\n","        y = self.layer3(y)\n","        y = self.layer4(y)\n","        y = self.layer5(y)\n","        y = y.reshape(y.size(0), -1)\n","        y = self.fc(y)\n","        y = self.fc1(y)\n","        y = self.fc2(y)\n","        return y"],"metadata":{"id":"UKVGjLQSWd8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = 10\n","num_epochs = 20\n","batch_size = 64\n","learning_rate = 0.005\n","\n","model = AlexNet(num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n","\n","# Train the model\n","total_step = len(train_loader)"],"metadata":{"id":"_OxUkCD-WlIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_step = len(train_loader)\n","\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","        # Move tensors to the configured device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","            \n","    # Validation\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for images, labels in valid_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            del images, labels, outputs\n","    \n","        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "],"metadata":{"id":"144dcqz2WnQe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        del images, labels, outputs\n","\n","    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"],"metadata":{"id":"LMYQLgV1WsrM"},"execution_count":null,"outputs":[]}]}